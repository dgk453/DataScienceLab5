{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E--IYPybViN7"
   },
   "source": [
    "# Data Science Lab: Lab 5 - Nishaanth Joopelli, Daniel Kim, Farzaan Hussain\n",
    "\n",
    "Submit:\n",
    "1. A pdf of your notebook with solutions.\n",
    "2. A link to your colab notebook or also upload your .ipynb if not working on colab.\n",
    "\n",
    "# Goals of this Lab\n",
    "\n",
    "1. Random Forests\n",
    "2. Boosting\n",
    "3. Playing with Ensembling packages, including XGBoost and CatBoost\n",
    "4. One more time: Revisiting CIFAR-10 and MNIST\n",
    "5. Getting ready for Kaggle\n",
    "\n",
    "We will soon open a Kaggle competition made for this class. In that one, you will be participating on your own. This is an intro to get us started, and also an excuse to work with regularization and regression which we have been discussing. You'll revisit some problems from earlier labs, this time using Random Forests, and Boosting. In particular, you should take this opportunity to become familiar with some very useful packages for boosting. I recommend not only the boosting packages in scikit-learn, but also XGBoost, GBM Light, CatBoost and possibly others. You have to download these and get them running, and then read their documentation to figure out how they work, what the hyperparameters are, etc.\n",
    "\n",
    "Also, the metric we will use in the Kaggle competition is AUC. We will discuss this. In the meantime, you may want to understand how it works. At least one key thing to remember: to get a good AUC score, you need to submit a soft score (probabilities) and not rounded values (i.e., not 0s and 1s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3789472/3722246774.py:11: DeprecationWarning: Please import `pearsonr` from the `scipy.stats` namespace; the `scipy.stats.stats` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  from scipy.stats.stats import pearsonr\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iegd555UZRJe"
   },
   "source": [
    "## Problem 1: Revisiting Logistic Regression and MNIST\n",
    "\n",
    "We have played with the handwriting recognition problem (the MNIST data set) using decision trees. We have also considered the same problem using multi-class Logistic Regression in a previous Lab. We revisit this one more time.\n",
    "\n",
    "**Part 1**: Use Random Forests to try to get the best possible *test accuracy* on MNIST. This involves getting acquainted with how Random Forests work, understanding their parameters, and therefore using Cross Validation to find the best settings. How well can you do? You should use the accuracy metric, since this is what you used in the previous Lab  -- therefore this will allow you to compare your results from Random Forests with your results from L1- and L2- Regularized Logistic Regression.\n",
    "\n",
    "What are the hyperparameters of your best model?\n",
    "\n",
    "**Part 2**: Use Boosting to do the same. Take the time to understand how XGBoost works (and/or other boosting packages available -- CatBoost is also another favorite). Try your best to tune your hyper-parameters. As added motivation: typically the winners and near-winners of the Kaggle competition are those that are best able to tune and cross validate XGBoost. What are the hyperparameters of your best model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyFVq4efZ33D"
   },
   "source": [
    "## Problem 2: Revisiting Logistic Regression and CIFAR-10\n",
    "\n",
    "Now that you have your pipeline set up, it should be easy to apply the above procedure to CIFAR-10. If you did something that takes significant computation time, keep in mind that CIFAR-10 is a few times larger.\n",
    "\n",
    "**Part 1**: What is the best accuracy you can get on the test data, by tuning Random Forests? What are the hyperparameters of your best model?\n",
    "\n",
    "**Part 2**: What is the best accuracy you can get on the test data, by tuning XGBoost? What are the hyperparameters of your best model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6XFMTH2aCRm"
   },
   "source": [
    "## Problem 3: Revisiting Kaggle\n",
    "\n",
    "This is a continuation of Problem 2 from Lab 3. You already did some first steps there, including making a Kaggle account, and trying ridge and lasso linear regression. You also tried stacking.\n",
    "\n",
    "**Part 1** (Nothing to hand in) Revisit Lab 3 and your answers there.\n",
    "\n",
    "**Part 2**: Train a gradient boosting regression, e.g., using XGBoost. What score can you get just from a single XGB? (you will need to optimize over its parameters).\n",
    "\n",
    "**Part 3**: Do your best to get a more accurate model. Try feature engineering and stacking many models. You are allowed to use any public tool in python. No non-python tools allowed. State what hyperparameters and models you tried, and the corresponding train/test error.\n",
    "\n",
    "**Part 4**: (Optional)  Read the Kaggle forums, tutorials and Kernels in this competition. This is an excellent way to learn. Include in your report if you find something in the forums you like, or if you made your own post or code post, especially if other Kagglers liked or used it afterwards.\n",
    "\n",
    "**Other**: Be sure to read and learn the rules of Kaggle! No sharing of code or data outside the Kaggle forums. Every student should have their own individual Kaggle account and teams can be formed in the Kaggle submissions with your Lab partner. This is more important for live competitions of course.\n",
    "\n",
    "In the real in-class Kaggle competition (which will be next), you will be graded based on your public score (include that in your report) and also on the creativity of your solution. In your report, due after the competition closes, you will explain what worked and what did not work. Many creative things will not work, but you will get partial credit for developing them. You can start thinking about this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GKQMEKaHRQxg"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "train.head()\n",
    "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                      test.loc[:,'MSSubClass':'SaleCondition']))\n",
    "prices = pd.DataFrame({\"price\":train[\"SalePrice\"], \"log(price + 1)\":np.log1p(train[\"SalePrice\"])})\n",
    "#log transform the target:\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "#log transform skewed numeric features:\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "skewed_feats = skewed_feats.index\n",
    "\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "all_data = pd.get_dummies(all_data)\n",
    "#filling NA's with the mean of the column:\n",
    "all_data = all_data.fillna(all_data.mean())\n",
    "#creating matrices for sklearn:\n",
    "X_train = all_data[:train.shape[0]]\n",
    "X_test = all_data[train.shape[0]:]\n",
    "y = train.SalePrice\n",
    "\n",
    "#Split into train and test data\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(X_train, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'subsample': 0.6, 'n_estimators': 700, 'min_child_weight': 1, 'max_depth': 5, 'learning_rate': 0.05, 'gamma': 0, 'colsample_bytree': 0.6}\n",
      "XGBoost Training Error: 0.0001\n",
      "XGBoost Test Error: 0.0175\n"
     ]
    }
   ],
   "source": [
    "# Part 2\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the parameter grid for hyperparameters\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 300, 500, 700],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost regressor\n",
    "xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV since GridSearch takes longer\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_reg,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # Number of parameter settings sampled\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_tr, y_tr)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "\n",
    "# Best estimator\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Predict on validation set\n",
    "y_train_pred_best = best_xgb.predict(X_tr)\n",
    "y_test_pred_best = best_xgb.predict(X_test)\n",
    "\n",
    "# Evaluate the optimized model\n",
    "training_error = mean_squared_error(y_tr, y_train_pred_best)\n",
    "test_error = mean_squared_error(y_test, y_test_pred_best)\n",
    "\n",
    "print(f\"XGBoost Training Error: {training_error:.4f}\")\n",
    "print(f\"XGBoost Test Error: {test_error:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1168, 200)\n",
      "X_test shape: (292, 200)\n",
      "y_train shape: (1168,)\n",
      "y_test shape: (292,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import skew\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Combine train and test data for preprocessing\n",
    "all_data = pd.concat((train.loc[:, 'MSSubClass':'SaleCondition'],\n",
    "                      test.loc[:, 'MSSubClass':'SaleCondition']))\n",
    "\n",
    "# Log-transform the target\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "# Log-transform skewed numeric features\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75].index\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "# One-hot encoding\n",
    "all_data = pd.get_dummies(all_data)\n",
    "\n",
    "# Fill missing values\n",
    "all_data = all_data.fillna(all_data.mean())\n",
    "\n",
    "# Split back into training and test sets\n",
    "X_full_train = all_data[:train.shape[0]]  \n",
    "X_full_test = all_data[train.shape[0]:]  \n",
    "y = train.SalePrice                       \n",
    "\n",
    "# Split X_full_train and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full_train, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Engineering Steps\n",
    "# Identify numerical features for interaction terms\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Select top 10 numerical features based on variance\n",
    "top_numerical = X_train[numerical_features].var().sort_values(ascending=False).head(10).index.tolist()\n",
    "\n",
    "# Function to create interaction terms used for both train and test data\n",
    "def create_interaction_terms(df, top_features):\n",
    "    interaction_terms = pd.DataFrame(index=df.index)\n",
    "    for pair in combinations(top_features, 2):\n",
    "        new_feature = f\"{pair[0]}_x_{pair[1]}\"\n",
    "        interaction_terms[new_feature] = df[pair[0]] * df[pair[1]]\n",
    "    return interaction_terms\n",
    "\n",
    "# Create interaction terms for training and testing data\n",
    "interaction_terms_train = create_interaction_terms(X_train, top_numerical)\n",
    "interaction_terms_test = create_interaction_terms(X_test, top_numerical)\n",
    "\n",
    "# Concatenate interaction terms back to the original data\n",
    "X_train = pd.concat([X_train, interaction_terms_train], axis=1)\n",
    "X_test = pd.concat([X_test, interaction_terms_test], axis=1)\n",
    "\n",
    "# Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "\n",
    "# Fit and transform on training data\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "\n",
    "# Transform testing data\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Get feature names for new DataFrame\n",
    "poly_features = poly.get_feature_names_out(X_train.columns)\n",
    "\n",
    "# Convert to DataFrame\n",
    "X_train_poly = pd.DataFrame(X_train_poly, columns=poly_features, index=X_train.index)\n",
    "X_test_poly = pd.DataFrame(X_test_poly, columns=poly_features, index=X_test.index)\n",
    "\n",
    "# Align columns of test data to training data\n",
    "X_test_poly = X_test_poly.reindex(columns=X_train_poly.columns, fill_value=0)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train_poly)\n",
    "X_test = imputer.transform(X_test_poly)\n",
    "\n",
    "# Feature Selection\n",
    "selector = SelectKBest(score_func=f_regression, k=200)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data\n",
    "X_train = selector.transform(X_train)\n",
    "X_test = selector.transform(X_test)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = np.array(poly_features)[selector.get_support()]\n",
    "\n",
    "# Optionally, convert to DataFrame (if you need feature names)\n",
    "X_train = pd.DataFrame(X_train, columns=selected_features)\n",
    "X_test = pd.DataFrame(X_test, columns=selected_features)\n",
    "\n",
    "# Convert targets to numpy arrays (if not already)\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# Check shapes to ensure consistency\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning for XGBoost...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Train Error: 0.0074, Test Error: 0.0218\n",
      ", Best_Parameter: {'alpha': '0.1958', 'colsample_bytree': '0.5347', 'gamma': '0.0504', 'lambda': '0.0182', 'learning_rate': '0.0383', 'max_depth': 5, 'min_child_weight': 7, 'n_estimators': 948, 'subsample': '0.8279'}\n",
      "\n",
      "Hyperparameter tuning for RandomForest...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "RandomForest - Train Error: 0.0044, Test Error: 0.0230\n",
      ", Best_Parameter: {'bootstrap': True, 'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 319}\n",
      "\n",
      "Hyperparameter tuning for GradientBoosting...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "GradientBoosting - Train Error: 0.0051, Test Error: 0.0219\n",
      ", Best_Parameter: {'alpha': '0.2848', 'learning_rate': '0.0211', 'max_depth': 3, 'min_samples_split': 3, 'n_estimators': 741, 'subsample': '0.7055'}\n",
      "\n",
      "All hyperparameters tried for XGBoost:\n",
      "    mean_test_score  std_test_score  mean_train_score  param_alpha  \\\n",
      "0           -0.0224          0.0044           -0.0163     0.374540   \n",
      "1           -0.0236          0.0041           -0.0151     0.333709   \n",
      "2           -0.0219          0.0032           -0.0024     0.617482   \n",
      "3           -0.0227          0.0046           -0.0163     0.592415   \n",
      "4           -0.0220          0.0040           -0.0117     0.385417   \n",
      "5           -0.0216          0.0035           -0.0118     0.258780   \n",
      "6           -0.0226          0.0039           -0.0164     0.395150   \n",
      "7           -0.0234          0.0046           -0.0166     0.388677   \n",
      "8           -0.0225          0.0041           -0.0148     0.074551   \n",
      "9           -0.0251          0.0042           -0.0189     0.790176   \n",
      "10          -0.0228          0.0035           -0.0110     0.063558   \n",
      "11          -0.0232          0.0045           -0.0155     0.713245   \n",
      "12          -0.0238          0.0038           -0.0171     0.475370   \n",
      "13          -0.0238          0.0048           -0.0169     0.289751   \n",
      "14          -0.0217          0.0036           -0.0138     0.324345   \n",
      "15          -0.0231          0.0031           -0.0153     0.304781   \n",
      "16          -0.0242          0.0030           -0.0145     0.518791   \n",
      "17          -0.0223          0.0042           -0.0155     0.284840   \n",
      "18          -0.0233          0.0041           -0.0174     0.144895   \n",
      "19          -0.0247          0.0047           -0.0188     0.803140   \n",
      "20          -0.0237          0.0039           -0.0141     0.695813   \n",
      "21          -0.0237          0.0045           -0.0172     0.940523   \n",
      "22          -0.0219          0.0036           -0.0120     0.695784   \n",
      "23          -0.0232          0.0042           -0.0170     0.897216   \n",
      "24          -0.0249          0.0050           -0.0185     0.887770   \n",
      "25          -0.0236          0.0029           -0.0147     0.283921   \n",
      "26          -0.0234          0.0041           -0.0163     0.325400   \n",
      "27          -0.0223          0.0042           -0.0130     0.243990   \n",
      "28          -0.0218          0.0037           -0.0135     0.668924   \n",
      "29          -0.0239          0.0047           -0.0160     0.953929   \n",
      "30          -0.0229          0.0050           -0.0153     0.853009   \n",
      "31          -0.0224          0.0045           -0.0126     0.930017   \n",
      "32          -0.0225          0.0042           -0.0143     0.136621   \n",
      "33          -0.0246          0.0044           -0.0181     0.511342   \n",
      "34          -0.0230          0.0047           -0.0169     0.578865   \n",
      "35          -0.0226          0.0042           -0.0117     0.822601   \n",
      "36          -0.0230          0.0034           -0.0139     0.051682   \n",
      "37          -0.0210          0.0040           -0.0123     0.433521   \n",
      "38          -0.0220          0.0034           -0.0142     0.156437   \n",
      "39          -0.0244          0.0044           -0.0182     0.657111   \n",
      "40          -0.0221          0.0027           -0.0031     0.116073   \n",
      "41          -0.0220          0.0032           -0.0056     0.464674   \n",
      "42          -0.0233          0.0045           -0.0160     0.539377   \n",
      "43          -0.0241          0.0055           -0.0173     0.573367   \n",
      "44          -0.0206          0.0034           -0.0073     0.195791   \n",
      "45          -0.0217          0.0042           -0.0129     0.385397   \n",
      "46          -0.0236          0.0050           -0.0177     0.877472   \n",
      "47          -0.0226          0.0037           -0.0128     0.731648   \n",
      "48          -0.0261          0.0041           -0.0199     0.754543   \n",
      "49          -0.0229          0.0040           -0.0144     0.958541   \n",
      "\n",
      "    param_colsample_bytree  param_gamma  param_lambda  param_learning_rate  \\\n",
      "0                 0.975357     0.365997      0.598658             0.056806   \n",
      "1                 0.571433     0.325444      0.056412             0.226600   \n",
      "2                 0.805827     0.003533      0.023062             0.167432   \n",
      "3                 0.523225     0.303772      0.170524             0.029515   \n",
      "4                 0.507983     0.115447      0.241025             0.214979   \n",
      "5                 0.831261     0.155856      0.520068             0.174013   \n",
      "6                 0.963329     0.363636      0.326541             0.181133   \n",
      "7                 0.635675     0.414369      0.356753             0.094280   \n",
      "8                 0.993443     0.386122      0.198716             0.011657   \n",
      "9                 0.802980     0.463150      0.651077             0.284488   \n",
      "10                0.655491     0.162592      0.729606             0.201267   \n",
      "11                0.880393     0.280639      0.770967             0.158139   \n",
      "12                0.781638     0.347758      0.139331             0.191325   \n",
      "13                0.580611     0.464849      0.808120             0.200021   \n",
      "14                0.561044     0.178149      0.906828             0.091640   \n",
      "15                0.582328     0.267045      0.484830             0.217731   \n",
      "16                0.851509     0.181815      0.971782             0.298734   \n",
      "17                0.518443     0.304782      0.502679             0.025444   \n",
      "18                0.744726     0.492825      0.242055             0.211641   \n",
      "19                0.735150     0.491712      0.398824             0.254930   \n",
      "20                0.929179     0.162979      0.220241             0.223345   \n",
      "21                0.698786     0.258876      0.837710             0.212707   \n",
      "22                0.614275     0.087477      0.982168             0.164991   \n",
      "23                0.950209     0.316551      0.339030             0.114763   \n",
      "24                0.925464     0.467817      0.785341             0.210696   \n",
      "25                0.652682     0.242807      0.448424             0.308337   \n",
      "26                0.873246     0.324816      0.849223             0.207284   \n",
      "27                0.986505     0.196549      0.892047             0.199342   \n",
      "28                0.932084     0.115093      0.499193             0.181601   \n",
      "29                0.957432     0.185079      0.015457             0.288496   \n",
      "30                0.647224     0.192549      0.851137             0.105077   \n",
      "31                0.535208     0.104459      0.671144             0.117594   \n",
      "32                0.854455     0.276410      0.296510             0.135934   \n",
      "33                0.750758     0.399148      0.649964             0.220590   \n",
      "34                0.719237     0.336013      0.328153             0.056512   \n",
      "35                0.680095     0.063530      0.522243             0.240998   \n",
      "36                0.765677     0.270318      0.637430             0.227827   \n",
      "37                0.872021     0.125430      0.184334             0.034262   \n",
      "38                0.625121     0.274613      0.714596             0.208059   \n",
      "39                0.717836     0.365020      0.047716             0.179811   \n",
      "40                0.523001     0.020364      0.855461             0.221097   \n",
      "41                0.824887     0.024029      0.949146             0.276004   \n",
      "42                0.841982     0.307926      0.943892             0.293275   \n",
      "43                0.564250     0.405602      0.820639             0.197782   \n",
      "44                0.534681     0.050389      0.018222             0.038333   \n",
      "45                0.840807     0.170312      0.260695             0.158811   \n",
      "46                0.867536     0.401740      0.282035             0.063232   \n",
      "47                0.563845     0.125008      0.580544             0.270135   \n",
      "48                0.551562     0.451276      0.505252             0.257937   \n",
      "49                0.923572     0.177453      0.956801             0.213031   \n",
      "\n",
      "    param_max_depth  param_min_child_weight  param_n_estimators  \\\n",
      "0                 5                       7                 430   \n",
      "1                 8                       2                 291   \n",
      "2                12                       3                 975   \n",
      "3                 6                       9                 415   \n",
      "4                14                       8                 134   \n",
      "5                 8                       2                 829   \n",
      "6                14                       9                 801   \n",
      "7                14                       9                 256   \n",
      "8                13                       3                 692   \n",
      "9                11                       8                 879   \n",
      "10               13                       3                 838   \n",
      "11               11                       3                 674   \n",
      "12               13                       4                 624   \n",
      "13                8                       4                 897   \n",
      "14               13                       9                 727   \n",
      "15               12                       8                 926   \n",
      "16                9                       2                 486   \n",
      "17                7                       1                 484   \n",
      "18               13                       8                 222   \n",
      "19                3                       9                 223   \n",
      "20                5                       4                 963   \n",
      "21               11                       9                 250   \n",
      "22                3                       2                 996   \n",
      "23                5                       4                 251   \n",
      "24                5                       9                 198   \n",
      "25                6                       1                 679   \n",
      "26                3                       3                 569   \n",
      "27                4                       5                 897   \n",
      "28                3                       1                 271   \n",
      "29                3                       5                 706   \n",
      "30                4                       9                 834   \n",
      "31               13                       2                 884   \n",
      "32                4                       9                 507   \n",
      "33               14                       3                 570   \n",
      "34                8                       6                 404   \n",
      "35               14                       5                 202   \n",
      "36                4                       3                 756   \n",
      "37               11                       5                 740   \n",
      "38                7                       1                 371   \n",
      "39                5                       5                 185   \n",
      "40               14                       5                 650   \n",
      "41                6                       3                 541   \n",
      "42               14                       4                 763   \n",
      "43                3                       1                 267   \n",
      "44                5                       7                 948   \n",
      "45               12                       5                 725   \n",
      "46               11                       8                 291   \n",
      "47                7                       1                 794   \n",
      "48                8                       3                 276   \n",
      "49                6                       3                 605   \n",
      "\n",
      "    param_subsample  \n",
      "0          0.729624  \n",
      "1          0.996106  \n",
      "2          0.757117  \n",
      "3          0.781644  \n",
      "4          0.954660  \n",
      "5          0.724877  \n",
      "6          0.662665  \n",
      "7          0.901098  \n",
      "8          0.855671  \n",
      "9          0.665449  \n",
      "10         0.559797  \n",
      "11         0.947882  \n",
      "12         0.538490  \n",
      "13         0.941640  \n",
      "14         0.676284  \n",
      "15         0.661601  \n",
      "16         0.650439  \n",
      "17         0.619781  \n",
      "18         0.621080  \n",
      "19         0.754099  \n",
      "20         0.548088  \n",
      "21         0.770724  \n",
      "22         0.546551  \n",
      "23         0.702254  \n",
      "24         0.986832  \n",
      "25         0.618625  \n",
      "26         0.632601  \n",
      "27         0.822552  \n",
      "28         0.970229  \n",
      "29         0.981810  \n",
      "30         0.922107  \n",
      "31         0.924335  \n",
      "32         0.956620  \n",
      "33         0.513808  \n",
      "34         0.518674  \n",
      "35         0.542674  \n",
      "36         0.699410  \n",
      "37         0.586647  \n",
      "38         0.758348  \n",
      "39         0.507197  \n",
      "40         0.673152  \n",
      "41         0.750520  \n",
      "42         0.838584  \n",
      "43         0.952675  \n",
      "44         0.827861  \n",
      "45         0.814471  \n",
      "46         0.787962  \n",
      "47         0.875436  \n",
      "48         0.526743  \n",
      "49         0.815919  \n",
      "\n",
      "================================================================================\n",
      "\n",
      "All hyperparameters tried for RandomForest:\n",
      "    mean_test_score  std_test_score  mean_train_score  param_bootstrap  \\\n",
      "0           -0.0239          0.0051           -0.0168             True   \n",
      "1           -0.0219          0.0044           -0.0112             True   \n",
      "2           -0.0366          0.0039           -0.0067            False   \n",
      "3           -0.0215          0.0042           -0.0077             True   \n",
      "4           -0.0353          0.0059           -0.0175            False   \n",
      "5           -0.0323          0.0071           -0.0195            False   \n",
      "6           -0.0321          0.0060           -0.0186            False   \n",
      "7           -0.0361          0.0045           -0.0046            False   \n",
      "8           -0.0223          0.0047           -0.0130             True   \n",
      "9           -0.0310          0.0059           -0.0180            False   \n",
      "10          -0.0361          0.0046           -0.0083            False   \n",
      "11          -0.0321          0.0060           -0.0186            False   \n",
      "12          -0.0245          0.0054           -0.0181             True   \n",
      "13          -0.0311          0.0051           -0.0146            False   \n",
      "14          -0.0231          0.0049           -0.0152             True   \n",
      "15          -0.0313          0.0060           -0.0185            False   \n",
      "16          -0.0336          0.0041           -0.0123            False   \n",
      "17          -0.0307          0.0059           -0.0170            False   \n",
      "18          -0.0214          0.0041           -0.0052             True   \n",
      "19          -0.0308          0.0053           -0.0162            False   \n",
      "20          -0.0247          0.0055           -0.0185             True   \n",
      "21          -0.0222          0.0043           -0.0093             True   \n",
      "22          -0.0223          0.0046           -0.0130             True   \n",
      "23          -0.0379          0.0054           -0.0024            False   \n",
      "24          -0.0215          0.0042           -0.0089             True   \n",
      "25          -0.0361          0.0045           -0.0046            False   \n",
      "26          -0.0236          0.0051           -0.0163             True   \n",
      "27          -0.0236          0.0051           -0.0163             True   \n",
      "28          -0.0238          0.0052           -0.0168             True   \n",
      "29          -0.0221          0.0045           -0.0122             True   \n",
      "30          -0.0217          0.0042           -0.0101             True   \n",
      "31          -0.0216          0.0041           -0.0094             True   \n",
      "32          -0.0214          0.0042           -0.0071             True   \n",
      "33          -0.0356          0.0039           -0.0103            False   \n",
      "34          -0.0347          0.0033           -0.0111            False   \n",
      "35          -0.0234          0.0050           -0.0157             True   \n",
      "36          -0.0220          0.0046           -0.0122             True   \n",
      "37          -0.0247          0.0055           -0.0185             True   \n",
      "38          -0.0323          0.0052           -0.0134            False   \n",
      "39          -0.0231          0.0050           -0.0152             True   \n",
      "40          -0.0219          0.0042           -0.0111             True   \n",
      "41          -0.0213          0.0039           -0.0046             True   \n",
      "42          -0.0371          0.0028           -0.0083            False   \n",
      "43          -0.0236          0.0051           -0.0166             True   \n",
      "44          -0.0229          0.0049           -0.0147             True   \n",
      "45          -0.0371          0.0028           -0.0083            False   \n",
      "46          -0.0322          0.0065           -0.0193            False   \n",
      "47          -0.0241          0.0053           -0.0172             True   \n",
      "48          -0.0315          0.0054           -0.0206            False   \n",
      "49          -0.0219          0.0044           -0.0111             True   \n",
      "\n",
      "   param_max_depth  param_min_samples_leaf  param_min_samples_split  \\\n",
      "0               23                      15                       12   \n",
      "1               24                       7                       12   \n",
      "2               24                       4                        9   \n",
      "3               25                       2                       13   \n",
      "4                5                       1                       13   \n",
      "5               15                      17                       11   \n",
      "6               19                      15                       16   \n",
      "7               26                       3                        6   \n",
      "8               24                       9                        8   \n",
      "9               28                      14                       19   \n",
      "10              24                       2                       16   \n",
      "11              11                      15                        4   \n",
      "12               7                      18                        9   \n",
      "13               9                      10                        5   \n",
      "14              21                      12                        3   \n",
      "15               7                      14                       17   \n",
      "16              17                       8                       17   \n",
      "17              18                      13                       10   \n",
      "18              16                       1                        8   \n",
      "19            None                      12                        9   \n",
      "20              14                      19                       18   \n",
      "21               6                       1                        6   \n",
      "22              29                       9                        8   \n",
      "23              15                       2                        2   \n",
      "24              26                       5                        4   \n",
      "25              25                       3                        2   \n",
      "26              18                      14                        4   \n",
      "27              29                      14                        8   \n",
      "28              18                      15                       11   \n",
      "29              22                       7                       18   \n",
      "30               7                       5                        8   \n",
      "31              14                       4                       14   \n",
      "32              22                       2                       11   \n",
      "33              28                       6                       13   \n",
      "34              14                       7                        2   \n",
      "35              28                      13                       10   \n",
      "36               9                       8                       10   \n",
      "37            None                      19                       11   \n",
      "38              18                       9                       18   \n",
      "39              29                      12                        8   \n",
      "40              20                       5                       18   \n",
      "41              20                       2                        3   \n",
      "42              26                       5                        2   \n",
      "43               5                      12                        7   \n",
      "44               7                      11                       18   \n",
      "45              27                       5                        3   \n",
      "46              14                      16                       17   \n",
      "47               9                      16                        4   \n",
      "48               7                      19                        4   \n",
      "49              23                       7                       10   \n",
      "\n",
      "    param_n_estimators  \n",
      "0                  171  \n",
      "1                  558  \n",
      "2                  763  \n",
      "3                  513  \n",
      "4                  413  \n",
      "5                  575  \n",
      "6                  662  \n",
      "7                  918  \n",
      "8                  373  \n",
      "9                  876  \n",
      "10                 554  \n",
      "11                 305  \n",
      "12                 487  \n",
      "13                 921  \n",
      "14                 301  \n",
      "15                 370  \n",
      "16                 848  \n",
      "17                 256  \n",
      "18                 620  \n",
      "19                 571  \n",
      "20                 491  \n",
      "21                 589  \n",
      "22                 300  \n",
      "23                 147  \n",
      "24                 783  \n",
      "25                 838  \n",
      "26                 868  \n",
      "27                 894  \n",
      "28                 991  \n",
      "29                 663  \n",
      "30                 848  \n",
      "31                 259  \n",
      "32                 656  \n",
      "33                 655  \n",
      "34                 484  \n",
      "35                 358  \n",
      "36                 417  \n",
      "37                 783  \n",
      "38                 929  \n",
      "39                 485  \n",
      "40                 180  \n",
      "41                 319  \n",
      "42                 484  \n",
      "43                 515  \n",
      "44                 866  \n",
      "45                 297  \n",
      "46                 708  \n",
      "47                 759  \n",
      "48                 610  \n",
      "49                 739  \n",
      "\n",
      "================================================================================\n",
      "\n",
      "All hyperparameters tried for GradientBoosting:\n",
      "    mean_test_score  std_test_score  mean_train_score  param_alpha  \\\n",
      "0           -0.0257          0.0033           -0.0000     0.374540   \n",
      "1           -0.0224          0.0035           -0.0000     0.445833   \n",
      "2           -0.0280          0.0045           -0.0000     0.020584   \n",
      "3           -0.0217          0.0030           -0.0000     0.183405   \n",
      "4           -0.0214          0.0032           -0.0000     0.611853   \n",
      "5           -0.0219          0.0034           -0.0000     0.785176   \n",
      "6           -0.0200          0.0032           -0.0007     0.170524   \n",
      "7           -0.0222          0.0038           -0.0000     0.015966   \n",
      "8           -0.0199          0.0029           -0.0006     0.833195   \n",
      "9           -0.0237          0.0023           -0.0000     0.311711   \n",
      "10          -0.0241          0.0039           -0.0000     0.842285   \n",
      "11          -0.0229          0.0043           -0.0000     0.326541   \n",
      "12          -0.0251          0.0045           -0.0000     0.271349   \n",
      "13          -0.0198          0.0032           -0.0000     0.165267   \n",
      "14          -0.0218          0.0033           -0.0000     0.815461   \n",
      "15          -0.0243          0.0045           -0.0000     0.926301   \n",
      "16          -0.0213          0.0032           -0.0001     0.330898   \n",
      "17          -0.0216          0.0032           -0.0000     0.274722   \n",
      "18          -0.0213          0.0033           -0.0000     0.721730   \n",
      "19          -0.0206          0.0034           -0.0000     0.025419   \n",
      "20          -0.0219          0.0037           -0.0000     0.695516   \n",
      "21          -0.0229          0.0038           -0.0000     0.228798   \n",
      "22          -0.0250          0.0040           -0.0000     0.633404   \n",
      "23          -0.0214          0.0035           -0.0000     0.883280   \n",
      "24          -0.0212          0.0033           -0.0000     0.227935   \n",
      "25          -0.0216          0.0029           -0.0000     0.164656   \n",
      "26          -0.0213          0.0038           -0.0000     0.244126   \n",
      "27          -0.0260          0.0038           -0.0000     0.971782   \n",
      "28          -0.0197          0.0031           -0.0033     0.284840   \n",
      "29          -0.0209          0.0032           -0.0031     0.033051   \n",
      "30          -0.0213          0.0033           -0.0001     0.985650   \n",
      "31          -0.0206          0.0031           -0.0000     0.728216   \n",
      "32          -0.0259          0.0031           -0.0000     0.816432   \n",
      "33          -0.0226          0.0022           -0.0000     0.590893   \n",
      "34          -0.0197          0.0027           -0.0004     0.645173   \n",
      "35          -0.0234          0.0044           -0.0000     0.837710   \n",
      "36          -0.0212          0.0041           -0.0000     0.695784   \n",
      "37          -0.0246          0.0032           -0.0000     0.996254   \n",
      "38          -0.0218          0.0031           -0.0000     0.339030   \n",
      "39          -0.0241          0.0035           -0.0000     0.161629   \n",
      "40          -0.0198          0.0030           -0.0022     0.663502   \n",
      "41          -0.0205          0.0030           -0.0000     0.994457   \n",
      "42          -0.0236          0.0037           -0.0000     0.746491   \n",
      "43          -0.0232          0.0038           -0.0000     0.542540   \n",
      "44          -0.0234          0.0026           -0.0000     0.393098   \n",
      "45          -0.0222          0.0031           -0.0000     0.576904   \n",
      "46          -0.0221          0.0038           -0.0000     0.768554   \n",
      "47          -0.0245          0.0033           -0.0001     0.747719   \n",
      "48          -0.0203          0.0031           -0.0000     0.980332   \n",
      "49          -0.0223          0.0038           -0.0000     0.372687   \n",
      "\n",
      "    param_learning_rate  param_max_depth  param_min_samples_split  \\\n",
      "0              0.295214               13                        9   \n",
      "1              0.039992               13                        5   \n",
      "2              0.300973               14                        7   \n",
      "3              0.101273                8                       13   \n",
      "4              0.051848               14                       17   \n",
      "5              0.069902                9                        4   \n",
      "6              0.029515                6                       15   \n",
      "7              0.079268               14                       16   \n",
      "8              0.062009                3                        5   \n",
      "9              0.166020               12                        5   \n",
      "10             0.144926               12                        5   \n",
      "11             0.181133               14                        9   \n",
      "12             0.258621               12                       14   \n",
      "13             0.014691               11                        2   \n",
      "14             0.222057                5                        4   \n",
      "15             0.205323               14                        8   \n",
      "16             0.029068                9                        6   \n",
      "17             0.178373                5                        6   \n",
      "18             0.080795                9                       10   \n",
      "19             0.042367                9                       18   \n",
      "20             0.051799                9                       14   \n",
      "21             0.033094               13                        3   \n",
      "22             0.271438               14                       13   \n",
      "23             0.107304                9                        8   \n",
      "24             0.138132                6                       14   \n",
      "25             0.170227                7                        2   \n",
      "26             0.060487               10                       10   \n",
      "27             0.298734                9                        3   \n",
      "28             0.021066                3                        3   \n",
      "29             0.113521                3                        3   \n",
      "30             0.082617                6                       12   \n",
      "31             0.120335                6                        3   \n",
      "32             0.249504               11                        7   \n",
      "33             0.213269                5                        4   \n",
      "34             0.062310                3                        9   \n",
      "35             0.212707               11                       19   \n",
      "36             0.078565               12                       17   \n",
      "37             0.299626                7                        6   \n",
      "38             0.114763                5                        2   \n",
      "39             0.279566                6                        4   \n",
      "40             0.011518                4                        3   \n",
      "41             0.062778                6                        2   \n",
      "42             0.204890                9                        4   \n",
      "43             0.162644                8                       15   \n",
      "44             0.277614               13                        3   \n",
      "45             0.157755                7                       17   \n",
      "46             0.023081               14                       15   \n",
      "47             0.295922                3                       15   \n",
      "48             0.032604                7                        2   \n",
      "49             0.128407                8                        4   \n",
      "\n",
      "    param_n_estimators  param_subsample  \n",
      "0                  800         0.798425  \n",
      "1                  971         0.854036  \n",
      "2                  485         0.590912  \n",
      "3                  956         0.645615  \n",
      "4                  882         0.728035  \n",
      "5                  584         0.803772  \n",
      "6                  341         0.692708  \n",
      "7                  554         0.804998  \n",
      "8                  661         0.831261  \n",
      "9                  921         0.515657  \n",
      "10                 369         0.863636  \n",
      "11                 824         0.694339  \n",
      "12                 140         0.648137  \n",
      "13                 747         0.502761  \n",
      "14                 388         0.802980  \n",
      "15                 300         0.811649  \n",
      "16                 198         0.795649  \n",
      "17                 561         0.924457  \n",
      "18                 306         0.713771  \n",
      "19                 663         0.781638  \n",
      "20                 754         0.877776  \n",
      "21                 781         0.904060  \n",
      "22                 655         0.708255  \n",
      "23                 289         0.555026  \n",
      "24                 844         0.652391  \n",
      "25                 918         0.634706  \n",
      "26                 791         0.681815  \n",
      "27                 486         0.650439  \n",
      "28                 741         0.705519  \n",
      "29                 152         0.744726  \n",
      "30                 283         0.618819  \n",
      "31                 297         0.699412  \n",
      "32                 563         0.520388  \n",
      "33                 610         0.613248  \n",
      "34                 739         0.758876  \n",
      "35                 250         0.770724  \n",
      "36                 671         0.630415  \n",
      "37                 259         0.816551  \n",
      "38                 727         0.542070  \n",
      "39                 211         0.550736  \n",
      "40                 921         0.724212  \n",
      "41                 497         0.662700  \n",
      "42                 500         0.654030  \n",
      "43                 769         0.986505  \n",
      "44                 376         0.751319  \n",
      "45                 758         0.786002  \n",
      "46                 472         0.941747  \n",
      "47                 280         0.786146  \n",
      "48                 355         0.742640  \n",
      "49                 971         0.535208  \n",
      "\n",
      "================================================================================\n",
      "\n",
      "Training Error: 0.00000000\n",
      "Testing Error: 0.0215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating all models\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats as stats\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# All the models I'm going to use\n",
    "xgb_reg = XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "rf_reg = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "gbr_reg = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "models = {\n",
    "    'XGBoost': xgb_reg,\n",
    "    'RandomForest': rf_reg,\n",
    "    'GradientBoosting': gbr_reg,\n",
    "}\n",
    "\n",
    "# Hyperparameter Tuning (used randomized hyperparameters within reasonable range)\n",
    "param_grids = {\n",
    "    'XGBoost': {\n",
    "        'n_estimators': stats.randint(100, 1000),\n",
    "        'max_depth': stats.randint(3, 15),\n",
    "        'learning_rate': stats.uniform(0.01, 0.3),\n",
    "        'subsample': stats.uniform(0.5, 0.5),\n",
    "        'colsample_bytree': stats.uniform(0.5, 0.5),\n",
    "        'gamma': stats.uniform(0, 0.5),\n",
    "        'min_child_weight': stats.randint(1, 10),\n",
    "        'alpha': stats.uniform(0, 1),  # L1 regularization (Lasso)\n",
    "        'lambda': stats.uniform(0, 1),  # L2 regularization (Ridge)\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': stats.randint(100, 1000),\n",
    "        'max_depth': [None] + list(range(5, 30)),\n",
    "        'min_samples_split': stats.randint(2, 20),\n",
    "        'min_samples_leaf': stats.randint(1, 20),\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': stats.randint(100, 1000),\n",
    "        'learning_rate': stats.uniform(0.01, 0.3),\n",
    "        'max_depth': stats.randint(3, 15),\n",
    "        'subsample': stats.uniform(0.5, 0.5),\n",
    "        'min_samples_split': stats.randint(2, 20),\n",
    "        'alpha': stats.uniform(0, 1),  # Adding L2 regularization to GradientBoosting\n",
    "    }\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "model_performances = {}\n",
    "hyperparameter_results = {}\n",
    "n_iter_search = 50  \n",
    "\n",
    "# Function to format parameters as floating points and not np.float\n",
    "def format_params(params):\n",
    "    formatted_params = {}\n",
    "    for key, value in params.items():\n",
    "        if isinstance(value, (float, np.float64, np.float32)):\n",
    "            formatted_params[key] = f\"{value:.4f}\"\n",
    "        else:\n",
    "            formatted_params[key] = value\n",
    "    return formatted_params\n",
    "\n",
    "# Finds best hyperparameters for each model\n",
    "for model_name in models:\n",
    "    # For tracking when each model is being hyperparameter tuned\n",
    "    print(f\"Hyperparameter tuning for {model_name}...\")\n",
    "    model = models[model_name]\n",
    "    param_grid = param_grids[model_name]\n",
    "    \n",
    "    # Initialize RandomizedSearchCV since GridSearch takes longer\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=n_iter_search,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        return_train_score=True #Needed to display all the hyperparameters used\n",
    "    )\n",
    "    \n",
    "    # Fit RandomizedSearchCV\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Store the best model\n",
    "    best_models[model_name] = random_search.best_estimator_\n",
    "\n",
    "    # Store hyperparameter results for reference for part 3\n",
    "    results_df = pd.DataFrame(random_search.cv_results_)\n",
    "    hyperparameter_results[model_name] = results_df\n",
    "    \n",
    "    # Predict on training and test sets\n",
    "    y_train_pred = random_search.predict(X_train)\n",
    "    y_test_pred = random_search.predict(X_test)\n",
    "    \n",
    "    # Calculate testing and training error\n",
    "    train_error = mean_squared_error(y_train, y_train_pred)\n",
    "    test_error = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Store performances for reference for part 3\n",
    "    model_performances[model_name] = {\n",
    "        'Best Parameters': random_search.best_params_,\n",
    "        'Training Error': train_error,\n",
    "        'Testing Error': test_error\n",
    "    }\n",
    "    \n",
    "    # Print the corresponding training error and test error for the best parameters for each model\n",
    "    print(f\"{model_name} - Train Error: {train_error:.4f}, Test Error: {test_error:.4f}\\n, Best_Parameter: {format_params(random_search.best_params_)}\\n\")\n",
    "\n",
    "# Stacking\n",
    "stacking_models = [\n",
    "    ('xgb', best_models['XGBoost']),\n",
    "    ('rf', best_models['RandomForest']),\n",
    "    ('gbr', best_models['GradientBoosting']),\n",
    "]\n",
    "\n",
    "# Initialize Stacking Regressor\n",
    "stacking_reg = StackingRegressor(\n",
    "    estimators=stacking_models,\n",
    "    final_estimator=Ridge(alpha=1.0, random_state=42),\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# Train Stacking Regressor\n",
    "stacking_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test sets\n",
    "y_train_pred_stack = stacking_reg.predict(X_train)\n",
    "y_test_pred_stack = stacking_reg.predict(X_test)\n",
    "\n",
    "# Calculate testing and training error\n",
    "train_error = mean_squared_error(y_train_pred_stack, y_train_pred_stack)\n",
    "test_error = mean_squared_error(y_test, y_test_pred_stack)\n",
    "\n",
    "# Loop over the selected models\n",
    "for model_name in list(models.keys()):\n",
    "    results_df = hyperparameter_results[model_name]\n",
    "    \n",
    "    # Select parameter columns and corresponding test and train error\n",
    "    columns_to_display = [\n",
    "        'mean_test_score', 'std_test_score', 'mean_train_score'\n",
    "    ] + [col for col in results_df.columns if col.startswith('param_')]\n",
    "    \n",
    "    # Round scores for models (these a)\n",
    "    results_df[['mean_test_score', 'std_test_score', 'mean_train_score']] = results_df[['mean_test_score', 'std_test_score', 'mean_train_score']].round(4)\n",
    "    \n",
    "    # Display all hyperparameters tried for the model\n",
    "    print(f\"All hyperparameters tried for {model_name}:\")\n",
    "    print(results_df[columns_to_display])\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"Training Error: {train_error:.8f}\")\n",
    "print(f\"Testing Error: {test_error:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DataScienceLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
